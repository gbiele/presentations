---
title: "Learning from experimental and observational data"
author: "Guido Biele"
format:
  revealjs:
    embed-resources: true
editor: source
---


# Background 

```{r}
library(dagitty)
source("dag_utils.R")
library(ggplot2)
library(patchwork)
theme_set(theme_light())
theme_update(text = element_text(size = 20))
par(mar=c(3,3,0,1), mgp=c(1,.5,0), tck=-.01)
```

```{css}
.reveal .title {
  font-size: 2em !important;
}
.reveal p {
  font-size: 0.8em !important;
}

```

## Bias

- Most scientific enterprises attempt to estimate some quantity or effect
- **Bias** is present if our estimate of that effect systematically deviates from the true value 
- When thinking about bias, it is important to clearly define the **estimand**: What is it that we are trying to estimate?
  - $bias = estimand - estimate$
  
## Estimate the right thing

A well described **estimand** is characterized by

- a clear effect measure
  - i.e., not just "Stress", but "Stress, operationalized as ... "
- a clear target population
  - for which set of humans do we want to measure the effect? 

## Internal and external validity

- If a study has internal validity, we correctly estimate the effect of a manipulation in the study sample
- If a study has external validity, the estimated effect in our study sample is the same as in the target population

<br> 

- **Studies with either internal or external validity can still be biased**
- **Internal and external validity alone are necessary but not sufficient conditions for obtaining unbiased estimates.**


## Bias in observational studies

Nobody needs convincing that confounding is a problem

:::: {.columns}
::: {.column width="48%"}
Observed confounders are not a problem, we can simply adjust.
```{r}
fork = dagitty(
  "dag{
  C->T;
  C->O;
  }")
coord.list = 
  list(
    x=c(T=0,O=2,C=1),
    y=c(T=0,O=0,C=-1))
coordinates(fork) = coord.list
adjustedNodes(fork)  = "C"
drawmydag(fork)
```
:::

::: {.column width="4%"}
<!-- empty column to create gap -->
:::

::: {.column width="48%"}
**Un**observed confounders are a problem! There is little we can do!
```{r}
fork = dagitty(
  "dag{
  U->T;
  U->O;
  }")
coord.list = 
  list(
    x=c(T=0,O=2,U=1),
    y=c(T=0,O=0,U=-1))
latents(fork) = "U"
coordinates(fork) = coord.list
drawmydag(fork)
```
:::
::::

**Treatment randomisation prevent confounding.** 

## Selection bias

```{r}
#| fig-align: "center"
collider = dagitty(
  "dag{
  T->S;
  L->S;
  L->O;
  }")
coord.list = 
  list(
    x=c(T=0,O=2,S=1,L=0),
    y=c(T=0,O=.5,S=.5,L=1))
coordinates(collider) = coord.list
adjustedNodes(collider)  = "S"
drawmydag(collider)
```

If the treatment determines who stays in the study and there is another variable L that causes who stays and the outcome, there will be selection bias

## Selection bias in obs. & exp. studies

:::: {.columns}
::: {.column width="48%"}
```{r}
collider = dagitty(
  "dag{
  Dist->FU;
  Edu->FU;
  Edu->Fit;
  }")
coord.list = 
  list(
    x=c(Dist=0,Fit=2,FU=1,Edu=0),
    y=c(Dist=0,Fit=.5,FU=.5,Edu=1))
coordinates(collider) = coord.list
latents(collider) = "Edu"
adjustedNodes(collider)  = "FU"
drawmydag(collider)
```
Estimating the effect of distance to work out place on fitness will be biased if participation in the follow up (FU) depends on distance, and unobserved education influences both FU & fitness. 
:::

::: {.column width="4%"}
<!-- empty column to create gap -->
:::

::: {.column width="48%"}
```{r}
collider = dagitty(
  "dag{
  Med->Si;
  Co->Si;
  Si->DO;
  Co->Sympt;
  }")
coord.list = 
  list(
    x=c(Med=0,Sympt=2,Si=.75,Co=0,DO=1.25),
    y=c(Med=0,Sympt=.5,Si=.5,Co=1,DO=.5))
coordinates(collider) = coord.list
latents(collider) = "Co"
adjustedNodes(collider)  = "DO"
drawmydag(collider)
```
An RCT where medication influences side effects (Si), which influence drop out (DO), and unobserved comorbid disorders influence Si & symptoms will produce a biased effect estimate.
:::
::::

**Experiments with drop out also require the assumption of no unobserved confounding!**

## Bias in experimental studies I
:::: {.columns}
::: {.column width="48%"}
```{r}
#| fig-width: 7
fork = dagitty(
  "dag{
  T->O;
  C->O;
  }")
coord.list = 
  list(
    x=c(T=0,O=1,C=0),
    y=c(T=0,O=0,C=1))
coordinates(fork) = coord.list
drawmydag(fork)
```
:::

::: {.column width="4%"}
<!-- empty column to create gap -->
:::

::: {.column width="48%"}
```{r}
#| fig-width: 7
fork = dagitty(
  "dag{
  Treat->Sympt;
  Age->Sympt;
  }")
coord.list = 
  list(
    x=c(Treat=0,Sympt=1,Age=0),
    y=c(Treat=0,Sympt=0,Age=1))
coordinates(fork) = coord.list
drawmydag(fork)
```
:::
::::

If there are some characteristics C that influence the effect of the treatment T on the outcome O (T-C-interaction), and the distribution of C is different in the study sample and target population, a randomised experiment will produce biased effect estimates. 

## "Bias" in experimental studies II
:::: {.columns}
::: {.column width="48%"}
```{r}
#| fig-width: 7
outcomes = dagitty(
  "dag{
  T->M;
  M->O_1;
  M->O_2
  }")
coord.list = 
  list(
    x=c(T=0,M=1,O_1=2,O_2=2),
    y=c(T=0,M=0,O_1=-1,O_2=1))
coordinates(outcomes) = coord.list
drawmydag(outcomes)
```
When the outcomes of interest $O_1$ and $O_2$ depend exclusively on a common mediator $M$ then results about the effect on $O_1$ will be informative about the effect on $O_2$
:::

::: {.column width="4%"}
<!-- empty column to create gap -->
:::

::: {.column width="48%"}
```{r}
#| fig-width: 7
outcomes = dagitty(
  "dag{
  T->O_1;
  T->O_2;
  X->O_2;
  }")
coord.list = 
  list(
    x=c(T=0,O_1=2,O_2=2,X=0),
    y=c(T=0,O_1=-1,O_2=1,X=1))
coordinates(outcomes) = coord.list
drawmydag(outcomes)
```
When only one of the outcomes depends on and additional variable $X$, results from experiment with outcome $O_1$ are a biased estimate for the effect on $O_2$.
:::
::::


## Translational research needs experiments and observation

- Results from observational studies can be biased
- But the same is true for experiments
- Experiments can be improved through random sampling of participants and investigation of multiple outcomes
- Even then, many experiments cannot be performed for ethical reasons

# Observational research is hard

## Does parental income influence children’s mental health?

:::: {.columns}
::: {.column width="30%"}
Kinge et al.,<b> Parental income and mental disorders in children and adolescents: prospective register-based study.</b> IJE (2021). <br> <small> "...associations between lower parental income and children’s mental disorders were partly, but not fully, attributed to other socio-demographic factors..." </small>
:::
::: {.column width="70%"}
![Results of Kinge et al. 2021 IJE](pngs/Kinge_A.png)
:::
::::


## Does parental income influence children’s mental health?

:::: {.columns}
::: {.column width="30%"}
Sariaslan et al., <b> No causal associations between childhood family income and subsequent psychiatric disorders, substance misuse and violent crime arrests: a nationwide Finnish study of >650 000 individuals and their siblings. </b> IJE (2021)
:::
::: {.column width="70%"}
![Results of Sariaslan et al. 2021 IJE](pngs/Sariaslan.png)
:::
::::

## Study designs

::: {.r-stack}
![](pngs/DAGdesigns1.png){.fragment}

![](pngs/DAGdesigns2.png){.fragment}

![](pngs/DAGdesigns3.png){.fragment}
:::
We analyse one data set to see in how far the results of Sariaslan and Kinge depend on modelling assumptions.
<br>
When the outcome is binary, sibling designs and panel designs with FE can only use a subset of data: Siblings and individuals with discordant outcomes, respectively.


## Samples sizes and prevalence

![](pngs/N_prev.png){width=150% fig-align="center"}
Choosing a particular study design introduces substantial differences between the study sample and the source and target population.


## Results: Odds & hazard ratios 
![](pngs/ORIncp1b.png)
Association is stronger when parents do not already have a diagnosis.

## Results: Odds & hazard ratios 
![](pngs/ORIncp2b.png)
Original results of Kinge at al (2021) and Sariaslan et al (2021) can be replicated in this data set.

## Results: Odds & hazard ratios 
![](pngs/ORIncp3b.png)
**Only a logistic (or cox) regression with adjustment finds an association.**

## Summary of results {background-image="pngs/bias.gif"  background-opacity="0.25"}

- We can replicate the results of Kinge et al. (_there is an association_) and Sariaslan et al. (_there is no association_) using the same basic data set
- Conditional logit regression also shows no effect 

Sibling design and fixed effects panel model make fewer assumptions / have a higher internal validity. Should we rather trust these results?

## Income variability

![](pngs/ICC.png){fig-align="center"}

Income variation in designs with high internal validity is only a fraction (~20%) of the income variability between families.

## Selection bias 
![](pngs/SIB_SB.png){fig-align="center"}

If parental income is a cause of # children & we are looking for a negative effect of income, sibling designs show a biased (attenuated) association. Sibling design studies have higher internal validity lower but external validity. 

## Conclusion

- Different results of Kinge et al. (2021) and Sariaslan et al. (2021) are likely due to the choice of different analysis approaches
- There is no categorical difference between "non-causal" and "causal" designs.
- Causal inference always needs assumptions, which should be scrutinized.


# Summary

- Causal inference is hard, RCTs do not guarantee unbiased causal effects
  - randomized treatment assignment is not enough
- Still, causal inference with observational data is harder
  - designs with higher internal validity:  Difference in difference, Regression discontinuity, Instrumental variables, sibling designs
  - But "causal designs" are no panacea: Each study requires careful consideration
- **Sensitivity analysis: What to the results look like if we assume some unobserved confounding**